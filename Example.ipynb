{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Damask** (ˈdæməsk; دمشق) is a reversible patterned fabric of silk, wool, linen, cotton, or synthetic fibers, with a pattern formed by weaving. \n",
    "\n",
    "This library, much like its namesake damask fabric, intertwines complexity and functionality into a seamless whole. \n",
    "\n",
    "Just as damask is known for its intricate, reversible patterns woven into a single piece of fabric, the Damask class weaves together text and annotations, allowing for rich, layered analysis without cutting or altering the original 'fabric' of the text. \n",
    "\n",
    "The library's ability to segment and annotate text non-destructively mirrors the way patterns in damask fabric are an integral part of its structure, rather than being merely printed or dyed on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, at core, a Damask is meant to be a drop in replacement for a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damask.models import Damask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myDamask = Damask(\n",
    "    \"Sitting on the dock of the bay, waiting for the ...\"\n",
    ")\n",
    "\n",
    "print(myDamask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load PaulGraham's essay in to a Damask to provide a more substantial piece of text to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"PaulGrahamEssay.txt\", \"r\")\n",
    "text = text.read()\n",
    "\n",
    "essay = Damask(text)\n",
    "\n",
    "print(essay[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of text processing, it's often necessary to divide large blocks of text into smaller segments. Typically, this process involves breaking the text into a new array of substrings, which can lead to the loss of the original text structure.\n",
    "\n",
    "However, Damask offers a non-destructive alternative. Instead of creating an array of substrings, Damask retains the entire original text and records the positions where splits occur. This way, you can access the individual segments without losing the context of the whole text. Damask provides an easy-to-use interface to interact with these segments, allowing you to handle large texts more effectively while preserving their integrity.\n",
    "\n",
    "The splitting is achieved via **Segmenters**\n",
    "\n",
    "Several are provided, but you can write your own custom logic, and apply it to a Damask by subclassing Segmenter and creating generators that parse the original text and identify start/end indices of your desired segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation with Damask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damask.segmenters import SentenceSegmenter, WordSegmenter, ChunkSegmenter\n",
    "\n",
    "split_into_sentences = SentenceSegmenter()\n",
    "split_into_words = WordSegmenter()\n",
    "split_into_chunks = ChunkSegmenter(1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Damask method: **segment_text** allows you to apply the segmenter function to the Damask and store this under a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay.segment_text(segmenter=split_into_sentences, annotation_type=\"sentences\")\n",
    "essay.segment_text(segmenter=split_into_words, annotation_type=\"words\")\n",
    "essay.segment_text(segmenter=split_into_chunks, annotation_type=\"chunks\")\n",
    "essay.segment_text(segmenter=ChunkSegmenter(512), annotation_type=\"chunks512\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single damask can maintain multiple different segments within the same structure\n",
    "- The original text is unchanged\n",
    "- You can divide the text into sentences, words or custom-sized chunks or create your own segmentation logic.\n",
    "- These divisions are not destructive - they are only created when you need them.\n",
    "- You can access and work with different segments at any time without losing the context of the original full text.\n",
    "\n",
    "you can call this via\n",
    "\n",
    "\\<your instance\\>.\\<your chunk key name\\>.texts (to get all the chunks)\n",
    "\n",
    "\\<your instance\\>.\\<your chunk key name\\>.annotations (to access the annotation and associated metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(essay.sentences.texts[0:10])\n",
    "print(essay.get_annotation_sets())\n",
    "print(essay.chunks.texts[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is achieved by keep track of each **Annotation** which is basically a segment with some metadata attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print just the first annotation for each annotation type\n",
    "\n",
    "print(essay.sentences.annotations[0])\n",
    "print(essay.words.annotations[0])\n",
    "print(essay.chunks.annotations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tag any metadata to the annotations - by default segmenting creates a basic annotation with a start, end index, and a metadata dictionary that contains a type name and a uuid.\n",
    "\n",
    "We're now going to use NTLK to annotate each sentence with a sentiment score.  We can use any python functionality in this annotation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from damask.annotators import PosAnnotator, SentimentAnnotator, LengthAnnotator\n",
    "from damask.annotators import EmbeddingAnnotator, ChatCompletionAnnotator\n",
    "essay.enrich_annotations(\n",
    "    enricher=SentimentAnnotator(), annotation_type=\"sentences\", parallel=True, workers=20\n",
    ")\n",
    "essay.enrich_annotations(\n",
    "    enricher=PosAnnotator(), annotation_type=\"words\", parallel=True, workers=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can embed \"sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "essay.enrich_annotations(\n",
    "    enricher=EmbeddingAnnotator(), annotation_type=\"sentences\", parallel=True, workers=40\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or use chat completion to list questions a \"chunk\" answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"You are a binary classifier that is given: A context, A user question and a Classifier task.\"\n",
    "user_prompt = \"\"\"Context: \\\"\\\"\\\"{text}\\\"\\\"\\\" \n",
    "User Question: \\\"What did the author do in summer of 2006?\\\"\n",
    "Task: If the context directly answers the question, return 1, and cite how it answers the question, linking context to quesiton.\n",
    "If not, return 0\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "essay.enrich_annotations(\n",
    "    enricher=ChatCompletionAnnotator(\n",
    "        system_prompt=prompt,\n",
    "        user_prompt=user_prompt,\n",
    "    ),\n",
    "    annotation_type=\"chunks\",\n",
    "    parallel=True,\n",
    "    workers=10,\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(essay.chunks.annotations)\n",
    "\"\"\"\n",
    "for chunk in essay.chunks.annotations:\n",
    "    if chunk.metadata[\"chat_completion\"] != \"0\":\n",
    "        print(chunk)\n",
    "        print(chunk.metadata)  # This will print the metadata dictionary for each chunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (essay.sentences.annotations[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (essay.words.annotations[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a straightforward method to display the contents of the annotation sets of the damask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (essay.annotation_sets_as_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\"\n",
    "counter = 1\n",
    "for sentence in essay.sentences.texts:\n",
    "    # skip empty sentences\n",
    "    if not sentence.strip():\n",
    "        continue\n",
    "    output += f\"<|{counter}|>{sentence}</|{counter}|>\\n\"\n",
    "    counter += 1\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
